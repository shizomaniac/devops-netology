1.  Узнайте о sparse (разряженных) файлах.

    Из википедии:
    Разрежённый файл — файл, в котором последовательности нулевых байтов[1] заменены на информацию об этих последовательностях (список дыр).
    Дыра — последовательность нулевых байт внутри файла, не записанная на диск. Информация о дырах (смещение от начала файла в байтах и количество байт) хранится в метаданных ФС.

    Как пример использования - thin provisioning в Vmware ESXi

2.  Могут ли файлы, являющиеся жесткой ссылкой на один объект, иметь разные права доступа и владельца? Почему?

    Не могут. Жесткие ссылки указывают на один и тот же inode поэтому и права будут одинаковыми

3.  Сделайте vagrant destroy на имеющийся инстанс Ubuntu. Замените содержимое Vagrantfile следующим:

Vagrant.configure("2") do |config|
  config.vm.box = "bento/ubuntu-20.04"
  config.vm.provider :virtualbox do |vb|
    lvm_experiments_disk0_path = "/tmp/lvm_experiments_disk0.vmdk"
    lvm_experiments_disk1_path = "/tmp/lvm_experiments_disk1.vmdk"
    vb.customize ['createmedium', '--filename', lvm_experiments_disk0_path, '--size', 2560]
    vb.customize ['createmedium', '--filename', lvm_experiments_disk1_path, '--size', 2560]
    vb.customize ['storageattach', :id, '--storagectl', 'SATA Controller', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', lvm_experiments_disk0_path]
    vb.customize ['storageattach', :id, '--storagectl', 'SATA Controller', '--port', 2, '--device', 0, '--type', 'hdd', '--medium', lvm_experiments_disk1_path]
  end
end
Данная конфигурация создаст новую виртуальную машину с двумя дополнительными неразмеченными дисками по 2.5 Гб.

4.  Используя fdisk, разбейте первый диск на 2 раздела: 2 Гб, оставшееся пространство.

vagrant@vagrant:~$ lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0                       7:0    0 61.9M  1 loop /snap/core20/1328
loop1                       7:1    0 43.6M  1 loop /snap/snapd/14978
loop2                       7:2    0 67.2M  1 loop /snap/lxd/21835
sda                         8:0    0   64G  0 disk
├─sda1                      8:1    0    1M  0 part
├─sda2                      8:2    0  1.5G  0 part /boot
└─sda3                      8:3    0 62.5G  0 part
  └─ubuntu--vg-ubuntu--lv 253:0    0 31.3G  0 lvm  /
sdb                         8:16   0  2.5G  0 disk
sdc                         8:32   0  2.5G  0 disk

root@vagrant:~# fdisk /dev/sdb
root@vagrant:~# lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0                       7:0    0 61.9M  1 loop /snap/core20/1328
loop1                       7:1    0 43.6M  1 loop /snap/snapd/14978
loop2                       7:2    0 67.2M  1 loop /snap/lxd/21835
sda                         8:0    0   64G  0 disk
├─sda1                      8:1    0    1M  0 part
├─sda2                      8:2    0  1.5G  0 part /boot
└─sda3                      8:3    0 62.5G  0 part
  └─ubuntu--vg-ubuntu--lv 253:0    0 31.3G  0 lvm  /
sdb                         8:16   0  2.5G  0 disk
├─sdb1                      8:17   0    2G  0 part
└─sdb2                      8:18   0  511M  0 part
sdc                         8:32   0  2.5G  0 disk


5.  Используя sfdisk, перенесите данную таблицу разделов на второй диск.

root@vagrant:~# sfdisk -d /dev/sdb | sfdisk /dev/sdc
root@vagrant:~# lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0                       7:0    0 61.9M  1 loop /snap/core20/1328
loop1                       7:1    0 43.6M  1 loop /snap/snapd/14978
loop2                       7:2    0 67.2M  1 loop /snap/lxd/21835
loop3                       7:3    0   62M  1 loop /snap/core20/1593
loop4                       7:4    0   47M  1 loop /snap/snapd/16292
loop5                       7:5    0 67.8M  1 loop /snap/lxd/22753
sda                         8:0    0   64G  0 disk
├─sda1                      8:1    0    1M  0 part
├─sda2                      8:2    0  1.5G  0 part /boot
└─sda3                      8:3    0 62.5G  0 part
  └─ubuntu--vg-ubuntu--lv 253:0    0 31.3G  0 lvm  /
sdb                         8:16   0  2.5G  0 disk
├─sdb1                      8:17   0    2G  0 part
└─sdb2                      8:18   0  511M  0 part
sdc                         8:32   0  2.5G  0 disk
├─sdc1                      8:33   0    2G  0 part
└─sdc2                      8:34   0  511M  0 part


6.  Соберите mdadm RAID1 на паре разделов 2 Гб.

root@vagrant:~# mdadm -C /dev/md1 --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1

7.  Соберите mdadm RAID0 на второй паре маленьких разделов.

root@vagrant:~# mdadm -C /dev/md0 --level=0 --raid-devices=2 /dev/sdb2 /dev/sdc2
root@vagrant:~# lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
loop0                       7:0    0   62M  1 loop  /snap/core20/1593
loop1                       7:1    0 67.2M  1 loop  /snap/lxd/21835
loop2                       7:2    0 67.8M  1 loop  /snap/lxd/22753
loop3                       7:3    0   47M  1 loop  /snap/snapd/16292
loop4                       7:4    0 43.6M  1 loop  /snap/snapd/14978
loop5                       7:5    0 61.9M  1 loop  /snap/core20/1328
sda                         8:0    0   64G  0 disk
├─sda1                      8:1    0    1M  0 part
├─sda2                      8:2    0  1.5G  0 part  /boot
└─sda3                      8:3    0 62.5G  0 part
  └─ubuntu--vg-ubuntu--lv 253:0    0 31.3G  0 lvm   /
sdb                         8:16   0  2.5G  0 disk
├─sdb1                      8:17   0    2G  0 part
│ └─md1                     9:1    0    2G  0 raid1
└─sdb2                      8:18   0  511M  0 part
  └─md0                     9:0    0 1018M  0 raid0
sdc                         8:32   0  2.5G  0 disk
├─sdc1                      8:33   0    2G  0 part
│ └─md1                     9:1    0    2G  0 raid1
└─sdc2                      8:34   0  511M  0 part
  └─md0                     9:0    0 1018M  0 raid0

8.  Создайте 2 независимых PV на получившихся md-устройствах.

root@vagrant:~# pvcreate /dev/md1
  Physical volume "/dev/md1" successfully created.
root@vagrant:~# pvcreate /dev/md0
  Physical volume "/dev/md0" successfully created.

9. Создайте общую volume-group на этих двух PV.

root@vagrant:~# vgcreate vg0 /dev/md0 /dev/md1
root@vagrant:~# vgcreate vg0 /dev/md0 /dev/md1
  Volume group "vg0" successfully created
root@vagrant:~# pvdisplay
  --- Physical volume ---
  PV Name               /dev/sda3
  VG Name               ubuntu-vg
  PV Size               <62.50 GiB / not usable 0
  Allocatable           yes
  PE Size               4.00 MiB
  Total PE              15999
  Free PE               8000
  Allocated PE          7999
  PV UUID               x7S6t2-at3n-E9kU-cz28-gAH3-QU9H-vyVuNf

  --- Physical volume ---
  PV Name               /dev/md0
  VG Name               vg0
  PV Size               1018.00 MiB / not usable 2.00 MiB
  Allocatable           yes
  PE Size               4.00 MiB
  Total PE              254
  Free PE               254
  Allocated PE          0
  PV UUID               JChzbg-zkbq-3xYp-HTf5-02rQ-0tR1-NmkEKA

  --- Physical volume ---
  PV Name               /dev/md1
  VG Name               vg0
  PV Size               <2.00 GiB / not usable 0
  Allocatable           yes
  PE Size               4.00 MiB
  Total PE              511
  Free PE               511
  Allocated PE          0
  PV UUID               FFY5Z0-e5EV-Ah9A-IhBt-NEpX-bxm3-8jnjMj


10. Создайте LV размером 100 Мб, указав его расположение на PV с RAID0.

root@vagrant:~# lvcreate -L 100m -n lv0 vg0 /dev/md0
  Logical volume "lv0" created.
root@vagrant:~# lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
loop0                       7:0    0   62M  1 loop  /snap/core20/1593
loop1                       7:1    0 67.2M  1 loop  /snap/lxd/21835
loop2                       7:2    0 67.8M  1 loop  /snap/lxd/22753
loop3                       7:3    0   47M  1 loop  /snap/snapd/16292
loop4                       7:4    0 43.6M  1 loop  /snap/snapd/14978
loop5                       7:5    0 61.9M  1 loop  /snap/core20/1328
sda                         8:0    0   64G  0 disk
├─sda1                      8:1    0    1M  0 part
├─sda2                      8:2    0  1.5G  0 part  /boot
└─sda3                      8:3    0 62.5G  0 part
  └─ubuntu--vg-ubuntu--lv 253:0    0 31.3G  0 lvm   /
sdb                         8:16   0  2.5G  0 disk
├─sdb1                      8:17   0    2G  0 part
│ └─md1                     9:1    0    2G  0 raid1
└─sdb2                      8:18   0  511M  0 part
  └─md0                     9:0    0 1018M  0 raid0
    └─vg0-lv0             253:1    0  100M  0 lvm
sdc                         8:32   0  2.5G  0 disk
├─sdc1                      8:33   0    2G  0 part
│ └─md1                     9:1    0    2G  0 raid1
└─sdc2                      8:34   0  511M  0 part
  └─md0                     9:0    0 1018M  0 raid0
    └─vg0-lv0             253:1    0  100M  0 lvm

11.  Создайте mkfs.ext4 ФС на получившемся LV.

root@vagrant:~# mkfs.ext4 /dev/vg0/lv0

12. Смонтируйте этот раздел в любую директорию, например, /tmp/new.

root@vagrant:~# mkfs.ext4 /dev/vg0/lv0
root@vagrant:~# mkdir /tmp/vagrant
root@vagrant:~# mount /dev/vg0/lv0 /tmp/vagrant
root@vagrant:~# cd /tmp/vagrant/

13. Поместите туда тестовый файл, например wget https://mirror.yandex.ru/ubuntu/ls-lR.gz -O /tmp/new/test.gz.

14. Прикрепите вывод lsblk.

root@vagrant:/tmp/vagrant# lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
loop0                       7:0    0   62M  1 loop  /snap/core20/1593
loop1                       7:1    0 67.2M  1 loop  /snap/lxd/21835
loop2                       7:2    0 67.8M  1 loop  /snap/lxd/22753
loop3                       7:3    0   47M  1 loop  /snap/snapd/16292
loop4                       7:4    0 43.6M  1 loop  /snap/snapd/14978
loop5                       7:5    0 61.9M  1 loop  /snap/core20/1328
sda                         8:0    0   64G  0 disk
├─sda1                      8:1    0    1M  0 part
├─sda2                      8:2    0  1.5G  0 part  /boot
└─sda3                      8:3    0 62.5G  0 part
  └─ubuntu--vg-ubuntu--lv 253:0    0 31.3G  0 lvm   /
sdb                         8:16   0  2.5G  0 disk
├─sdb1                      8:17   0    2G  0 part
│ └─md1                     9:1    0    2G  0 raid1
│   └─vg0-lv0             253:1    0  100M  0 lvm   /tmp/vagrant
└─sdb2                      8:18   0  511M  0 part
  └─md0                     9:0    0 1018M  0 raid0
sdc                         8:32   0  2.5G  0 disk
├─sdc1                      8:33   0    2G  0 part
│ └─md1                     9:1    0    2G  0 raid1
│   └─vg0-lv0             253:1    0  100M  0 lvm   /tmp/vagrant
└─sdc2                      8:34   0  511M  0 part
  └─md0                     9:0    0 1018M  0 raid0

15. Протестируйте целостность файла:

root@vagrant:/tmp/vagrant# gzip -t test.gz
root@vagrant:/tmp/vagrant# echo $?
0

16. Используя pvmove, переместите содержимое PV с RAID0 на RAID1.

root@vagrant:/tmp/vagrant# pvmove /dev/md0 /dev/md1

17. Сделайте --fail на устройство в вашем RAID1 md.

root@vagrant:/tmp/vagrant# mdadm --fail /dev/md1 /dev/sdb1
mdadm: set /dev/sdb1 faulty in /dev/md1


18. Подтвердите выводом dmesg, что RAID1 работает в деградированном состоянии.

root@vagrant:/tmp/vagrant# dmesg
[ 5225.424489] md/raid1:md1: Disk failure on sdb1, disabling device.
               md/raid1:md1: Operation continuing on 1 devices.
19. Протестируйте целостность файла, несмотря на "сбойный" диск он должен продолжать быть доступен:

root@vagrant:/tmp/vagrant# gzip -t test.gz
root@vagrant:/tmp/vagrant# echo $?
0
root@vagrant:~# gzip -t /tmp/new/test.gz
root@vagrant:~# echo $?
0

20. Погасите тестовый хост, vagrant destroy
